{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compressed-commerce",
   "metadata": {},
   "source": [
    "# Generation of the recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-australia",
   "metadata": {},
   "source": [
    "In this notebook, we will generate all the recordings with MEArec that will be necessary to populate the study and compare the sorters. First, we need to create a function that will, given a dictionary of parameter, generate a single recording. The recording parameters can be defined as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "curious-moldova",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import MEArec as mr\n",
    "import spikeinterface.full as si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9a07701",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../utils/')\n",
    "\n",
    "from corr_spike_trains import CorrelatedSpikeGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "suitable-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_params = {\n",
    "    'probe' : 'Neuronexus-32', #layout of the probe used\n",
    "    'duration' : 30*60, #total duration of the recording\n",
    "    'n_cell' : 20, # number of cells that will be injected\n",
    "    'fs' : 30000., # sampling rate\n",
    "    'lag_time' : 0.002,  # half refractory period in ms\n",
    "    'make_plots' : True,\n",
    "    'generate_recording' : True,\n",
    "    'noise_level' : 5,\n",
    "    'templates_seed' : 42,\n",
    "    'noise_seed' : 42,\n",
    "    'global_path' : os.path.abspath('../'),\n",
    "    'study_number' : 0,\n",
    "    'save_plots' : True,\n",
    "    'method' : 'brette', # 'poisson' | 'brette'\n",
    "    'corr_level' : 0,\n",
    "    'rate_level' : 5, #Hz\n",
    "    'nb_recordings' : 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-surname",
   "metadata": {},
   "source": [
    "With these parameters, we will create 20 neurons, and correlation levels will be generated via the mixture process of [Brette et al, 2009]. The function to generate a single recording is defined as follows. It assumes that you have, in your folder, a file named `../data/templates/templates_{probe}_100.h5` with all the pre-generated templates that will be used by MEArec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lightweight-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_recording(params=generation_params):\n",
    "\n",
    "    paths = {}\n",
    "    paths['basedir'] = params['global_path']\n",
    "    paths['data'] = None\n",
    "\n",
    "    if paths['data'] == None:\n",
    "        paths['data'] = os.path.join(paths['basedir'], 'data')\n",
    "\n",
    "    paths['templates'] =  os.path.join(paths['data'], 'templates')\n",
    "    paths['recordings'] = os.path.join(paths['data'], 'recordings') \n",
    "\n",
    "    for i in paths.values():\n",
    "        if not os.path.exists(i):\n",
    "            os.makedirs(i)\n",
    "\n",
    "    probe = params['probe']\n",
    "    n_cell = params['n_cell']\n",
    "    noise_level = params['noise_level']\n",
    "    study_number = params['study_number']\n",
    "    corr_level = params['corr_level']\n",
    "    rate_level = params['rate_level']\n",
    "\n",
    "    template_filename = os.path.join(paths['templates'], f'templates_{probe}_100.h5')\n",
    "    recording_filename = os.path.join(paths['recordings'], f'rec{study_number}_{n_cell}cells_{noise_level}noise_{corr_level}corr_{rate_level}rate_{probe}.h5')\n",
    "    plot_filename = os.path.join(paths['recordings'], f'rec{study_number}_{n_cell}cells_{noise_level}noise_{corr_level}corr_{rate_level}rate_{probe}.pdf')\n",
    "\n",
    "    spikerate = params['rate_level']\n",
    "    n_spike_alone = int(spikerate * params['duration'])\n",
    "\n",
    "    print('Total target rate:', params['rate_level'], \"Hz\")\n",
    "    print('Basal rate:', spikerate, \"Hz\")\n",
    "\n",
    "\n",
    "    # collision lag range\n",
    "    lag_sample = int(params['lag_time'] * params['fs'])\n",
    "\n",
    "    refactory_period = 2 * params['lag_time']\n",
    "\n",
    "    spiketimes = []\n",
    "\n",
    "    if params['method'] == 'poisson':\n",
    "        print('Spike trains generated as independent poisson sources')\n",
    "        \n",
    "        for i in range(params['n_cell']):\n",
    "            \n",
    "            #~ n = n_spike_alone + n_collision_by_pair * (params['n_cell'] - i - 1)\n",
    "            n = n_spike_alone\n",
    "            #~ times = np.random.rand(n_spike_alone) * params['duration']\n",
    "            times = np.random.rand(n) * params['duration']\n",
    "            \n",
    "            times = np.sort(times)\n",
    "            spiketimes.append(times)\n",
    "\n",
    "    elif params['method'] == 'brette':\n",
    "        print('Spike trains generated as compound mixtures')\n",
    "        C = np.ones((params['n_cell'], params['n_cell']))\n",
    "        C = params['corr_level'] * np.maximum(C, C.T)\n",
    "        #np.fill_diagonal(C, 0*np.ones(params['n_cell']))\n",
    "\n",
    "        rates = rates = params['rate_level']*np.ones(params['n_cell'])\n",
    "\n",
    "        cor_spk = CorrelatedSpikeGenerator(C, rates, params['n_cell'])\n",
    "        cor_spk.find_mixture(iter=1e4)\n",
    "        res = cor_spk.mixture_process(tauc=refactory_period/2, t=params['duration'])\n",
    "        \n",
    "        # make neo spiketrains\n",
    "        for i in range(params['n_cell']):\n",
    "            #~ print(spiketimes[i])\n",
    "            mask = res[:, 0] == i\n",
    "            times = res[mask, 1]\n",
    "            times = np.sort(times)\n",
    "            mask = (times > 0) * (times < params['duration'])\n",
    "            times = times[mask]\n",
    "            spiketimes.append(times)\n",
    "\n",
    "\n",
    "    # remove refactory period\n",
    "    for i in range(params['n_cell']):\n",
    "        times = spiketimes[i]\n",
    "        ind, = np.nonzero(np.diff(times) < refactory_period)\n",
    "        ind += 1\n",
    "        times = np.delete(times, ind)\n",
    "        assert np.sum(np.diff(times) < refactory_period) ==0\n",
    "        spiketimes[i] = times\n",
    "\n",
    "    # make neo spiketrains\n",
    "    spiketrains = []\n",
    "    for i in range(params['n_cell']):\n",
    "        mask = np.where(spiketimes[i] > 0)\n",
    "        spiketimes[i] = spiketimes[i][mask] \n",
    "        spiketrain = neo.SpikeTrain(spiketimes[i], units='s', t_start=0*pq.s, t_stop=params['duration']*pq.s)\n",
    "        spiketrain.annotate(cell_type='E')\n",
    "        spiketrains.append(spiketrain)\n",
    "\n",
    "    # check with sanity plot here\n",
    "    if params['make_plots']:\n",
    "        \n",
    "        # count number of spike per units\n",
    "        fig, axs = plt.subplots(2, 2)\n",
    "        count = [st.size for st in spiketrains]\n",
    "        ax = axs[0, 0]\n",
    "        simpleaxis(ax)\n",
    "        pairs = []\n",
    "        collision_count_by_pair = []\n",
    "        collision_count_by_units = np.zeros(n_cell)\n",
    "        for i in range(n_cell):\n",
    "            for j in range(i+1, n_cell):\n",
    "                times1 = spiketrains[i].rescale('s').magnitude\n",
    "                times2 = spiketrains[j].rescale('s').magnitude\n",
    "                matching_event = make_matching_events((times1*params['fs']).astype('int64'), (times2*params['fs']).astype('int64'), lag_sample)\n",
    "                pairs.append(f'{i}-{j}')\n",
    "                collision_count_by_pair.append(matching_event.size)\n",
    "                collision_count_by_units[i] += matching_event.size\n",
    "                collision_count_by_units[j] += matching_event.size\n",
    "        ax.plot(np.arange(len(collision_count_by_pair)), collision_count_by_pair)\n",
    "        ax.set_xticks(np.arange(len(collision_count_by_pair)))\n",
    "        ax.set_xticklabels(pairs)\n",
    "        ax.set_ylim(0, max(collision_count_by_pair) * 1.1)\n",
    "        ax.set_ylabel('# Collisions')\n",
    "        ax.set_xlabel('Pairs')\n",
    "\n",
    "        # count number of spike per units\n",
    "        count_total = np.array([st.size for st in spiketrains])\n",
    "        count_not_collision = count_total - collision_count_by_units\n",
    "\n",
    "        ax = axs[1, 0]\n",
    "        simpleaxis(ax)\n",
    "        ax.bar(np.arange(n_cell).astype(np.int)+1, count_not_collision, color='g')\n",
    "        ax.bar(np.arange(n_cell).astype(np.int)+1, collision_count_by_units, bottom =count_not_collision, color='r')\n",
    "        ax.set_ylabel('# spikes')\n",
    "        ax.set_xlabel('Cell id')\n",
    "        ax.legend(('Not colliding', 'Colliding'), loc='best')\n",
    "\n",
    "        # cross corrlogram\n",
    "        ax = axs[0, 1]\n",
    "        simpleaxis(ax)\n",
    "        counts = []\n",
    "        for i in range(n_cell):\n",
    "            for j in range(i+1, n_cell):\n",
    "                times1 = spiketrains[i].rescale('s').magnitude\n",
    "                times2 = spiketrains[j].rescale('s').magnitude\n",
    "                matching_event = make_matching_events((times1*params['fs']).astype('int64'), (times2*params['fs']).astype('int64'), lag_sample)\n",
    "                \n",
    "                #~ ax = axs[i, j]\n",
    "                all_lag = matching_event['delta_frame']  / params['fs']\n",
    "                count, bins  = np.histogram(all_lag, bins=np.arange(-params['lag_time'], params['lag_time'], params['lag_time']/20))\n",
    "                #~ ax.bar(bins[:-1], count, bins[1] - bins[0])\n",
    "                ax.plot(1000*bins[:-1], count, bins[1] - bins[0], c='0.5')\n",
    "                counts += [count]\n",
    "        counts = np.array(counts)\n",
    "        counts = np.mean(counts, 0)\n",
    "        ax.plot(1000*bins[:-1], counts, bins[1] - bins[0], c='r')\n",
    "        ax.set_xlabel('Lags [ms]')\n",
    "        ax.set_ylabel('# Collisions')\n",
    "\n",
    "        ax = axs[1, 1]\n",
    "        simpleaxis(ax)\n",
    "        ratios = []\n",
    "        for i in range(n_cell):\n",
    "            nb_spikes = len(spiketrains[i])\n",
    "            nb_collisions = 0\n",
    "            times1 = spiketrains[i].rescale('s').magnitude\n",
    "            for j in list(range(0, i)) + list(range(i+1, n_cell)):\n",
    "                times2 = spiketrains[j].rescale('s').magnitude\n",
    "                matching_event = make_matching_events((times1*params['fs']).astype('int64'), (times2*params['fs']).astype('int64'), lag_sample)\n",
    "                nb_collisions += matching_event.size\n",
    "\n",
    "            if nb_collisions > 0:\n",
    "                ratios += [nb_spikes / nb_collisions]\n",
    "            else:\n",
    "                ratios += [0]\n",
    "\n",
    "        ax.bar([0], [np.mean(ratios)], yerr=[np.std(ratios)])\n",
    "        ax.set_ylabel('# spikes / # collisions')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if params['save_plots']:\n",
    "            plt.savefig(plot_filename)\n",
    "        else:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    if params['generate_recording']:\n",
    "        spgen = mr.SpikeTrainGenerator(spiketrains=spiketrains)\n",
    "        rec_params = mr.get_default_recordings_params()\n",
    "        rec_params['recordings']['fs'] = params['fs']\n",
    "        rec_params['recordings']['sync_rate'] = None\n",
    "        rec_params['recordings']['sync_jitter'] = 5\n",
    "        rec_params['recordings']['noise_level'] = params['noise_level']\n",
    "        rec_params['recordings']['filter'] = False\n",
    "        rec_params['spiketrains']['duration'] = params['duration']\n",
    "        rec_params['spiketrains']['n_exc'] = params['n_cell']\n",
    "        rec_params['spiketrains']['n_inh'] = 0\n",
    "        rec_params['recordings']['chunk_duration'] = 10.\n",
    "        rec_params['templates']['n_overlap_pairs'] = None\n",
    "        rec_params['templates']['min_dist'] = 0\n",
    "        rec_params['seeds']['templates'] = params['templates_seed']\n",
    "        rec_params['seeds']['noise'] = params['noise_seed']\n",
    "        recgen = mr.gen_recordings(params=rec_params, spgen=spgen, templates=template_filename, verbose=True)\n",
    "        mr.save_recording_generator(recgen, filename=recording_filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-factor",
   "metadata": {},
   "source": [
    "Once this function is created, we can create an additional function that will generate several recordings, with different suffix/seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "weird-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recordings(params=generation_params):\n",
    "    for i in range(params['nb_recordings']):\n",
    "        generation_params['study_number'] = i\n",
    "        generation_params['templates_seed'] = i\n",
    "        generation_params['noise_seed'] = i\n",
    "        generate_single_recording(generation_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-metro",
   "metadata": {},
   "source": [
    "And now, we have all the required tools to create our recordings. By default, they will all be saved in the folder ../recordings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "false-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Provide the different rate and correlations levels you want to generate\n",
    "rate_levels = [5, 10, 15]\n",
    "corr_levels = [0, 0.1, 0.2]\n",
    "generation_params['nb_recordings'] = 5 #Number of recordings per conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "\n",
    "for rate_level in rate_levels:\n",
    "    for corr_level in corr_levels:\n",
    "\n",
    "        generation_params['rate_level'] = rate_level\n",
    "        generation_params['corr_level'] = corr_level\n",
    "        generate_recordings(generation_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-client",
   "metadata": {},
   "source": [
    "## Generation of the study objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-messaging",
   "metadata": {},
   "source": [
    "Since the recordings have been generated, we now need to create Study objects for spikeinterface, and run the sorters on all these recordings. Be careful that by default, this can create quite a large amount of data, if you have numerous rate/correlation levels and/or number of recordings and/or sorters. First, we need to tell spikeinterface how to find the sorters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "heavy-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "ironclust_path = '/media/cure/Secondary/pierre/softwares/ironclust'\n",
    "kilosort1_path = '/media/cure/Secondary/pierre/softwares/Kilosort-1.0'\n",
    "kilosort2_path = '/media/cure/Secondary/pierre/softwares/Kilosort-2.0'\n",
    "kilosort3_path = '/media/cure/Secondary/pierre/softwares/Kilosort-3.0'\n",
    "hdsort_path = '/media/cure/Secondary/pierre/softwares/HDsort'\n",
    "os.environ[\"KILOSORT_PATH\"] = kilosort1_path\n",
    "os.environ[\"KILOSORT2_PATH\"] = kilosort2_path\n",
    "os.environ[\"KILOSORT3_PATH\"] = kilosort3_path\n",
    "os.environ['IRONCLUST_PATH'] = ironclust_path\n",
    "os.environ['HDSORT_PATH'] = hdsort_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-weekend",
   "metadata": {},
   "source": [
    "And then we need to create a function that will, given a list of recordings, create a study and run all the sorters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "specific-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_study(params, keep_data=True):\n",
    "    paths = {}\n",
    "    paths['basedir'] = params['global_path']\n",
    "    paths['data'] = None\n",
    "\n",
    "    if paths['data'] == None:\n",
    "        paths['data'] = os.path.join(paths['basedir'], 'data')\n",
    "\n",
    "    paths['templates'] =  os.path.join(paths['data'], 'templates')\n",
    "    paths['recordings'] = os.path.join(paths['data'], 'recordings')\n",
    "    paths['study'] = os.path.join(paths['data'], 'study')\n",
    "    \n",
    "    for i in paths.values():\n",
    "        if not os.path.exists(i):\n",
    "            os.makedirs(i)\n",
    "\n",
    "    probe = params['probe']\n",
    "    n_cell = params['n_cell']\n",
    "    noise_level = params['noise_level']\n",
    "    study_number = params['study_number']\n",
    "    corr_level = params['corr_level']\n",
    "    rate_level = params['rate_level']\n",
    "\n",
    "    paths['mearec_filename'] = []\n",
    "\n",
    "    study_folder = os.path.join(paths['study'], f'{n_cell}cells_{noise_level}noise_{corr_level}corr_{rate_level}rate_{probe}')\n",
    "    study_folder = Path(study_folder)\n",
    "\n",
    "    if params['reset_study'] and os.path.exists(study_folder):\n",
    "        shutil.rmtree(study_folder)\n",
    "\n",
    "    print('Availables sorters:')\n",
    "    si.print_sorter_versions()\n",
    "\n",
    "    gt_dict = {}\n",
    "\n",
    "    if not os.path.exists(study_folder):\n",
    "\n",
    "        for i in range(params['nb_recordings']):\n",
    "            paths['mearec_filename'] += [os.path.join(paths['recordings'], f'rec{i}_{n_cell}cells_{noise_level}noise_{corr_level}corr_{rate_level}rate_{probe}.h5')]\n",
    "\n",
    "        print('Availables recordings:')\n",
    "        print(paths['mearec_filename'])\n",
    "\n",
    "        \n",
    "        for count, file in enumerate(paths['mearec_filename']):\n",
    "            rec  = si.MEArecRecordingExtractor(file)\n",
    "            sorting_gt = si.MEArecSortingExtractor(file)\n",
    "            gt_dict['rec%d' %count] = (rec, sorting_gt)\n",
    "\n",
    "        study = si.GroundTruthStudy.create(study_folder, gt_dict, n_jobs=-1, chunk_memory='1G', progress_bar=True)\n",
    "        study.run_sorters(params['sorter_list'], verbose=False, docker_images=params['docker_images'])\n",
    "        print(\"Study created!\")\n",
    "    else:\n",
    "        study = si.GroundTruthStudy(study_folder)\n",
    "        if params['relaunch'] == 'all':\n",
    "            if_exist = 'overwrite'\n",
    "        elif params['relaunch'] == 'some':\n",
    "            if_exist = 'keep'\n",
    "\n",
    "        if params['relaunch'] in ['all', 'some']:\n",
    "            study.run_sorters(params['sorter_list'], verbose=False, mode_if_folder_exists=if_exist, docker_images=params['docker_images'])\n",
    "            print(\"Study loaded!\")\n",
    "\n",
    "    study.copy_sortings()\n",
    "\n",
    "    if not keep_data:\n",
    "\n",
    "        for sorter in params['sorter_list']:\n",
    "\n",
    "            for rec in ['rec%d' %i for i in range(params['nb_recordings'])]:\n",
    "                sorter_path = os.path.join(study_folder, 'sorter_folders', rec, sorter)\n",
    "                if os.path.exists(sorter_path):\n",
    "                    for f in os.listdir(sorter_path):\n",
    "                        if f != 'spikeinterface_log.json':\n",
    "                            full_file = os.path.join(sorter_path, f)\n",
    "                            try:\n",
    "                                if os.path.isdir(full_file):\n",
    "                                    shutil.rmtree(full_file)\n",
    "                                else:\n",
    "                                    os.remove(full_file)\n",
    "                            except Exception:\n",
    "                                pass\n",
    "        for file in paths['mearec_filename']:\n",
    "            os.remove(file)\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-habitat",
   "metadata": {},
   "source": [
    "This function will take a dictionary of inputs (the same as for generating the recordings), and looping over all the possible recordings for a given condition (probe, rate, correlation levels) it will create a study in the path ../study/, running all the sorters on the recordings. This can take a lot of time, depending on the number of recordings/sorters. Note also that by default, the original recorindgs generated by MEArec are kept, and thus duplicated in the study folder. If you want to delete the original recordings (they are not needed for further analysis) you can set keep_data=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "independent-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_params = generation_params.copy()\n",
    "study_params['sorter_list'] = ['yass', 'kilosort', 'kilosort2', 'kilosort3', 'spykingcircus', 'tridesclous', 'ironclust', 'herdingspikes', 'hdsort']\n",
    "study_params['docker_images'] = {'yass' : 'spikeinterface/yass-base:2.0.0'} #If some sorters are installed via docker\n",
    "study_params['relaunch'] = 'all' #If you want to relaunch the sorters. \n",
    "study_params['reset_study'] = False #If you want to reset the study (delete everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_studies = {}\n",
    "for rate_level in rate_levels:\n",
    "    for corr_level in corr_levels:\n",
    "\n",
    "        study_params['rate_level'] = rate_level\n",
    "        study_params['corr_level'] = corr_level\n",
    "        all_studies[corr_level, rate_level] = generate_study(study_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-fleet",
   "metadata": {},
   "source": [
    "And this is it! Now you should have several studies, each of them with several recordings that have be analyzed by several sorters, in a structured manner (as function of rate/correlations levels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nikola": {
   "category": "",
   "date": "2021-11-29 10:28:53 UTC+01:00",
   "description": "",
   "link": "",
   "slug": "collision-paper-generate-recordings",
   "tags": "collision_paper_2021",
   "title": "Collision paper generate recordings",
   "type": "text"
  },  
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
