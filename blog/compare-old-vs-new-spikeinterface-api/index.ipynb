{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare \"old\" vs \"new \" spikeinterface API\n",
    "\n",
    "Author : Samuel Garcia\n",
    "29 March 2021\n",
    "\n",
    "In spring 2021, the spikeinterface team plan a \"big refactoring\" of the spikeinterface tool suite.\n",
    "\n",
    "Main changes are:\n",
    "  \n",
    "  * use neo as much as possible for extractors\n",
    "  * handle multi segment\n",
    "  * improve performance (pre and post processing)\n",
    "  * add A WaveformExtractor class\n",
    "\n",
    "Here I will benchmark 2 aspects of the \"new API\":\n",
    "  * filter with 10 workers on a multi core machine\n",
    "  * extractor waveform 1 worker vs 10 workers\n",
    "\n",
    "The becnhmark is done a 10 min spikeglx file with 384 channels.\n",
    "\n",
    "The sorting is done with kilosort3.\n",
    "\n",
    "My machine is Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz\n",
    "2 CPU with 20 core each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base_folder = Path('/mnt/data/sam/DataSpikeSorting/eduarda_arthur') \n",
    "data_folder = base_folder / 'raw_awake'"
   ]
  },
  {
   "source": [
    "## Filter with OLD API\n",
    "\n",
    "Here we :\n",
    "\n",
    "  1. open the file\n",
    "  2. lazy filter\n",
    "  3. cache it\n",
    "  4. dump to json\n",
    "\n",
    "The \"cache\" step is in fact the \"compute and save\" step."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "spikeextractors version 0.9.5\n",
      "spiketoolkit version 0.7.4\n",
      "<spiketoolkit.preprocessing.bandpass_filter.BandpassFilterRecording object at 0x7f648d3ee130>\n",
      "Old spikeextractors cache 801.9439885600004\n"
     ]
    }
   ],
   "source": [
    "import spikeextractors as se\n",
    "import spiketoolkit as st\n",
    "\n",
    "print('spikeextractors version', se.__version__)\n",
    "print('spiketoolkit version', st.__version__)\n",
    "\n",
    "# step 1: open\n",
    "file_path = data_folder / 'raw_awake_01_g0_t0.imec0.ap.bin'\n",
    "recording = se.SpikeGLXRecordingExtractor(file_path)\n",
    "\n",
    "# step 2: lazy filter\n",
    "rec_filtered = st.preprocessing.bandpass_filter(recording,  freq_min=300. freq_max=6000.)\n",
    "print(rec_filtered)\n",
    "\n",
    "save_folder = base_folder / 'raw_awake_filtered_old'\n",
    "if save_folder.is_dir():\n",
    "    shutil.rmtree(save_folder)\n",
    "save_folder.mkdir()\n",
    "\n",
    "save_file = save_folder / 'filetred_recording.dat'\n",
    "dump_file = save_folder / 'filetred_recording.json'\n",
    "\n",
    "# step 3: cache\n",
    "t0 = time.perf_counter()\n",
    "cached = se.CacheRecordingExtractor(rec_filtered, chunk_mb=50, n_jobs=10, \n",
    "    save_path=save_file)\n",
    "t1 = time.perf_counter()\n",
    "run_time_filter_old = t1-t0\n",
    "print('Old spikeextractors cache', run_time_filter_old)\n",
    "\n",
    "# step : dump\n",
    "cached.dump_to_json(dump_file)"
   ]
  },
  {
   "source": [
    "## Filter with NEW API\n",
    "\n",
    "Here we :\n",
    "\n",
    "  1. open the file\n",
    "  2. lazy filter\n",
    "  3. save it\n",
    "\n",
    "The \"save\" step is in fact the \"compute and save\" step."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "spikeinterface version 0.90.0\n",
      "SpikeGLXRecordingExtractor: 385 channels - 1 segments - 30.0kHz\n",
      "BandpassFilterRecording: 385 channels - 1 segments - 30.0kHz\n",
      "write_binary_recording with n_jobs 10  chunk_size 3246\n",
      "write_binary_recording: 100%|██████████| 5546/5546 [00:51<00:00, 108.39it/s]\n",
      "New spikeinterface filter + save binary 54.79437772196252\n"
     ]
    }
   ],
   "source": [
    " \n",
    "import spikeinterface as si\n",
    "\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.toolkit as st\n",
    "print('spikeinterface version', si.__version__)\n",
    "\n",
    "# step 1: open\n",
    "recording = se.SpikeGLXRecordingExtractor(data_folder)\n",
    "print(recording)\n",
    "\n",
    "# step 2: lazy filter\n",
    "rec_filtered =st.bandpass_filter(recording,  freq_min=300., freq_max=6000.)\n",
    "print(rec_filtered)\n",
    "\n",
    "\n",
    "filter_path = base_folder / 'raw_awake_filtered'\n",
    "if filter_path.is_dir():\n",
    "    shutil.rmtree(filter_path)\n",
    "\n",
    "# step 3 : compute and save with 10 workers\n",
    "t0 = time.perf_counter()\n",
    "cached = rec_filtered.save(folder=filter_path,\n",
    "    format='binary', dtype='int16',\n",
    "    n_jobs=10,  total_memory=\"50M\", progress_bar=True)\n",
    "t1 = time.perf_counter()\n",
    "run_time_filter_new = t1 -t0\n",
    "print('New spikeinterface filter + save binary', run_time_filter_new)"
   ]
  },
  {
   "source": [
    "## Extract waveform with OLD API\n",
    "\n",
    "Here we use get_unit_waveforms from toolkit.\n",
    "\n",
    "We do the computation with 1 and then 10 jobs.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "spikeextractors version 0.9.5\nspiketoolkit version 0.7.4\n"
     ]
    }
   ],
   "source": [
    "from spikeextractors.baseextractor import BaseExtractor\n",
    "import spikeextractors as se\n",
    "import spiketoolkit as st\n",
    "print('spikeextractors version', se.__version__)\n",
    "print('spiketoolkit version', st.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OLD API get_unit_waveforms 1 jobs 513.5964983040467\n"
     ]
    }
   ],
   "source": [
    "save_folder = base_folder / 'raw_awake_filtered_old'\n",
    "dump_file = save_folder / 'filetred_recording.json'\n",
    "recording = BaseExtractor.load_extractor_from_json(dump_file)\n",
    "\n",
    "sorting_KS3 = se.KiloSortSortingExtractor(base_folder / 'output_kilosort3')\n",
    "waveform_folder = base_folder / 'waveforms_extractor_old_1_job'\n",
    "if waveform_folder.is_dir():\n",
    "    shutil.rmtree(waveform_folder)\n",
    "waveform_folder.mkdir()\n",
    "sorting_KS3.set_tmp_folder(waveform_folder)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "wf, indexes, channels = st.postprocessing.get_unit_waveforms(recording, sorting_KS3,\n",
    "            max_spikes_per_unit=500, return_idxs=True, chunk_mb=50, n_jobs=1,\n",
    "            memmap=True)\n",
    "t1 = time.perf_counter()\n",
    "run_time_waveform_old_1jobs = t1 - t0\n",
    "print('OLD API get_unit_waveforms 1 jobs', run_time_waveform_old_1jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of chunks: 553 - Number of jobs: 10\n",
      "Impossible to delete temp file: /mnt/data/sam/DataSpikeSorting/eduarda_arthur/waveforms_extractor_old_10_jobs Error [Errno 16] Device or resource busy: '.nfs0000000004ce04d3000007b8'\n",
      "OLD API get_unit_waveforms 10 jobs 823.8002076600096\n"
     ]
    }
   ],
   "source": [
    "save_folder = base_folder / 'raw_awake_filtered_old'\n",
    "dump_file = save_folder / 'filetred_recording.json'\n",
    "recording = BaseExtractor.load_extractor_from_json(dump_file)\n",
    "\n",
    "sorting_KS3_bis = se.KiloSortSortingExtractor(base_folder / 'output_kilosort3')\n",
    "waveform_folder = base_folder / 'waveforms_extractor_old_10_jobs_'\n",
    "if waveform_folder.is_dir():\n",
    "    shutil.rmtree(waveform_folder)\n",
    "waveform_folder.mkdir()\n",
    "sorting_KS3_bis.set_tmp_folder(waveform_folder)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "wf, indexes, channels = st.postprocessing.get_unit_waveforms(recording, sorting_KS3_bis,\n",
    "            max_spikes_per_unit=500, return_idxs=True, chunk_mb=500, n_jobs=10,\n",
    "            memmap=True, verbose=True)\n",
    "t1 = time.perf_counter()\n",
    "run_time_waveform_old_10jobs = t1 - t0\n",
    "print('OLD API get_unit_waveforms 10 jobs', run_time_waveform_old_10jobs)"
   ]
  },
  {
   "source": [
    "## Extract waveform with NEW API\n",
    "\n",
    "The spikeinterface 0.9 API introduce more flexible object WaveformExtractor to do the same (extract snipet).\n",
    "\n",
    "Here some code example and benchmark speed.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "spikeinterface version 0.90.0\n",
      "KiloSortSortingExtractor: 184 units - 1 segments - 30.0kHz\n"
     ]
    }
   ],
   "source": [
    "import spikeinterface.extractors as se\n",
    "from spikeinterface import WaveformExtractor, load_extractor\n",
    "print('spikeinterface version', si.__version__)\n",
    "\n",
    "filter_path = base_folder / 'raw_awake_filtered'\n",
    "filered_recording = load_extractor(filter_path)\n",
    "\n",
    "sorting_KS3 = se.KiloSortSortingExtractor(base_folder / 'output_kilosort3')\n",
    "print(sorting_KS3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|##########| 278/278 [01:42<00:00,  2.72it/s]\n",
      "New WaveformExtractor 1 jobs 115.03656197001692\n"
     ]
    }
   ],
   "source": [
    "# 1 worker\n",
    "waveform_folder = base_folder / 'waveforms_extractor_1_job_new_'\n",
    "if waveform_folder.is_dir():\n",
    "    shutil.rmtree(waveform_folder)\n",
    "we = WaveformExtractor.create(filered_recording, sorting_KS3, waveform_folder)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "we.set_params(ms_before=3., ms_after=4., max_spikes_per_unit=500)\n",
    "we.run(n_jobs=1, total_memory=\"50M\", progress_bar=True)\n",
    "t1 = time.perf_counter()\n",
    "run_time_waveform_new_1jobs = t1 - t0\n",
    "print('New WaveformExtractor 1 jobs',run_time_waveform_new_1jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 278/278 [00:31<00:00,  8.87it/s]\n",
      "New WaveformExtractor 10 jobs 48.819815920025576\n"
     ]
    }
   ],
   "source": [
    "# 1 worker\n",
    "waveform_folder = base_folder / 'waveforms_extractor_10_job_new_'\n",
    "if waveform_folder.is_dir():\n",
    "    shutil.rmtree(waveform_folder)\n",
    "we = WaveformExtractor.create(filered_recording, sorting_KS3, waveform_folder)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "we.set_params(ms_before=3., ms_after=4., max_spikes_per_unit=500)\n",
    "we.run(n_jobs=10, total_memory=\"500M\", progress_bar=True)\n",
    "t1 = time.perf_counter()\n",
    "run_time_waveform_new_10jobs = t1 - t0\n",
    "print('New WaveformExtractor 10 jobs', run_time_waveform_new_10jobs)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Conclusion\n",
    "\n",
    "For filter with 10 workers the speedup is x14.\n",
    "\n",
    "For waveform extactor with 1 workers the speedup is x4\n",
    "\n",
    "For waveform extactor with 10 workers the speedup is x16\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "speedup filter 14.635515939778026\n"
     ]
    }
   ],
   "source": [
    "speedup_filter = run_time_filter_old / run_time_filter_new\n",
    "print('speedup filter', speedup_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "speedup waveforms 1 jobs 4.464637064152789\nspeedup waveformd 10jobs 16.874299751754943\n"
     ]
    }
   ],
   "source": [
    "speedup_waveform_1jobs = run_time_waveform_old_1jobs / run_time_waveform_new_1jobs\n",
    "print('speedup waveforms 1 jobs', speedup_waveform_1jobs)\n",
    "\n",
    "speedup_waveform_10jobs = run_time_waveform_old_10jobs / run_time_waveform_new_10jobs\n",
    "print('speedup waveformd 10jobs', speedup_waveform_10jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "env": {},
   "interrupt_mode": "signal",
   "language": "python",
   "name": "python3"
  },
  "nikola": {
   "category": "",
   "date": "2021-03-29 15:22:08 UTC+02:00",
   "description": "",
   "link": "",
   "slug": "compare-old-vs-new-spikeinterface-api",
   "tags": "new_api",
   "title": "Compare old vs new spikeinterface API",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}